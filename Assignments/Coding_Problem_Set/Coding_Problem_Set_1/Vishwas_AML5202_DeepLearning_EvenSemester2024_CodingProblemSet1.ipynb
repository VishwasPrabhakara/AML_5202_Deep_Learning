{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1705379494400,"user":{"displayName":"S N S Acharya","userId":"14786945180387920086"},"user_tz":-330},"id":"5dEgRpy3952M"},"outputs":[],"source":["## Load libraries\n","import numpy as np\n","import sys\n","import matplotlib.pyplot as plt\n","import matplotlib.cm as cm\n","plt.style.use('dark_background')\n","%matplotlib inline"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1705379494401,"user":{"displayName":"S N S Acharya","userId":"14786945180387920086"},"user_tz":-330},"id":"_y89MeGgxb2T"},"outputs":[],"source":["np.set_printoptions(precision = 2)"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"4T7eUtw7Mh0z"},"outputs":[{"name":"stdout","output_type":"stream","text":["WARNING:tensorflow:From c:\\Users\\vp140\\.conda\\envs\\pycaretenv\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n","\n"]}],"source":["import tensorflow as tf"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"Q1e2N5S8MlCU"},"outputs":[{"data":{"text/plain":["'2.15.0'"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["tf.__version__"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"B4EjOi-OM4Gp"},"outputs":[{"name":"stdout","output_type":"stream","text":["[[5 5 3 9 9 4 8 4 3 8]\n"," [4 9 5 9 7 4 8 8 6 6]\n"," [6 3 6 9 7 9 4 4 6 3]\n"," [3 4 3 3 9 4 7 8 6 3]\n"," [6 7 3 5 4 8 8 5 8 9]]\n","------\n","[2 0 2 0 1 0 1 1 1 0]\n","------\n","[[0. 0. 1.]\n"," [1. 0. 0.]\n"," [0. 0. 1.]\n"," [1. 0. 0.]\n"," [0. 1. 0.]\n"," [1. 0. 0.]\n"," [0. 1. 0.]\n"," [0. 1. 0.]\n"," [0. 1. 0.]\n"," [1. 0. 0.]]\n"]}],"source":["# Generate artificial data with 5 samples, 4 features per sample\n","# and 3 output classes\n","num_samples = 10 # number of samples\n","num_features = 5 # number of features (a.k.a. dimensionality)\n","num_labels = 3 # number of output labels\n","# Data matrix (each column = single sample)\n","X = np.random.choice(np.arange(3, 10), size = (num_features, num_samples), replace = True)\n","# Class labels\n","y = np.random.choice([0, 1, 2], size = num_samples, replace = True)\n","print(X)\n","print('------')\n","print(y)\n","print('------')\n","# One-hot encode class labels\n","y = tf.keras.utils.to_categorical(y)\n","print(y)"]},{"cell_type":"markdown","metadata":{"id":"IrXipxwrJ0_8"},"source":["---\n","\n","A generic layer class with forward and backward methods\n","\n","----"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"N4pKUhCyMrWm"},"outputs":[],"source":["class Layer:\n","  def __init__(self):\n","    self.input = None\n","    self.output = None\n","\n","  def forward(self, input):\n","    pass\n","\n","  def backward(self, output_gradient, learning_rate):\n","    pass"]},{"cell_type":"markdown","metadata":{"id":"vdLfiQSlOSUU"},"source":["---\n","\n","The softmax classifier steps for a generic sample $\\mathbf{x}$ with (one-hot encoded) true label $\\mathbf{y}$ (3 possible categories) using a randomly initialized weights matrix (with bias abosrbed as its last last column):\n","\n","1. Calculate raw scores vector for a generic sample $\\mathbf{x}$  (bias feature added): $$\\mathbf{z} = \\mathbf{Wx}.$$\n","2. Calculate softmax probabilities (that is, softmax-activate the raw scores) $$\\mathbf{a} = \\text{softmax}(\\mathbf{z})\\Rightarrow\\begin{bmatrix}a_0\\\\a_1\\\\a_2\\end{bmatrix}= \\text{softmax}\\left(\\begin{bmatrix}z_0\\\\z_1\\\\z_2\\end{bmatrix}\\right)=\\begin{bmatrix}\\frac{e^{z_0}}{e^{z_0}+e^{z_1}+e^{z_2}}\\\\\\frac{e^{z_1}}{e^{z_0}+e^{z_1}+e^{z_2}}\\\\\\frac{e^{z_2}}{e^{z_0}+e^{z_1}+e^{z_2}}\\end{bmatrix}$$\n","3. Softmax loss for this sample is (where output label $y$ is not yet one-hot encoded)\n","$$\\begin{align*}L &=  -\\log([a]_y) \\\\&= -\\log\\left(\\left[\\text{softmax}(\\mathbf{z})\\right]_y\\right)\\\\ &= -\\log\\left(\\left[\\text{softmax}(\\mathbf{Wx})\\right]_y\\right).\\end{align*}$$\n","4. Predicted probability vector that the sample belongs to each one of the output categories is given a new name $$\\hat{\\mathbf{y}} = \\mathbf{a}.$$\n","5. One-hot encoding the output label $$\\underbrace{y\\rightarrow\\mathbf{y}}_{\\text{e.g.}\\,2\\,\\rightarrow\\begin{bmatrix}0\\\\0\\\\1\\end{bmatrix}}$$ results in the following representation for the softmax loss for the sample which is also referred to as the categorical crossentropy (CCE) loss:\n","$$\\begin{align*}L &= L\\left(\\mathbf{y},\\hat{\\mathbf{y}}\\right)=\\sum_{k=0}^2-y_k\\log\\left(\\hat{y}_k\\right)\\end{align*}.$$\n","5. Calculate the gradient of the loss for the sample w.r.t. weights by following the computation graph from top to bottom (that is, backward):\n","$$\\begin{align*} L\\\\{\\color{yellow}\\downarrow}\\\\ \\hat{\\mathbf{y}} &= \\mathbf{a}\\\\{\\color{yellow}\\downarrow}\\\\\\mathbf{z}\\\\{\\color{yellow}\\downarrow}\\\\\\mathbf{W}\\end{align*}$$\n","$$\\begin{align*}\\Rightarrow \\nabla_\\mathbf{W}(L) &= \\nabla_\\mathbf{W}(\\mathbf{z}) \\times\\nabla_\\mathbf{z}(\\mathbf{a})\\times\\nabla_\\mathbf{a}(L)\\\\&= \\underbrace{\\nabla_\\mathbf{W}(\\mathbf{z})}_\\text{first term} \\times\\underbrace{\\nabla_\\mathbf{z}(\\mathbf{a})}_\\text{second to last term}\\times\\underbrace{\\nabla_\\hat{\\mathbf{y}}(L)}_\\text{last term}.\\end{align*}$$\n","7. Now focus on the last term $\\nabla_\\hat{\\mathbf{y}}(L)$:\n","$$\\begin{align*}\\nabla_\\hat{\\mathbf{y}}(L) &=\\begin{bmatrix}\\nabla_{\\hat{y}_0}(L)\\\\\\nabla_{\\hat{y}_1}(L)\\\\\\nabla_{\\hat{y}_2}(L)\\end{bmatrix} = \\begin{bmatrix}-y_0/\\hat{y}_0\\\\-y_1/\\hat{y}_1\\\\-y_2/\\hat{y}_2.\\end{bmatrix}\\end{align*}$$\n","8. Now focus on the second to last term $\\nabla_\\mathbf{z}(\\mathbf{a})$:\n","$$\\begin{align*}\\nabla_\\mathbf{z}(\\mathbf{a}) &= \\nabla_\\mathbf{z}\\left(\\begin{bmatrix}a_0\\\\a_1\\\\a_2\\end{bmatrix}\\right)\\\\ &= \\begin{bmatrix}\\nabla_\\mathbf{z}(a_0)&\\nabla_\\mathbf{z}(a_1)&\\nabla_\\mathbf{z}(a_2)\\end{bmatrix} \\\\&= \\begin{bmatrix}\\nabla_{z_0}(a_0)&\\nabla_{z_0}(a_1)&\\nabla_{z_0}(a_2)\\\\\\nabla_{z_1}(a_0)&\\nabla_{z_1}(a_1)&\\nabla_{z_1}(a_2)\\\\\\nabla_{z_2}(a_0)&\\nabla_{z_2}(a_1)&\\nabla_{z_2}(a_2)\\end{bmatrix}\\\\&=\\begin{bmatrix}a_0(1-a_0)&-a_1a_0&-a_2a_0\\\\-a_0a_1&a_1(1-a_1)&-a_2a_1\\\\-a_0a_2&-a_1a_2&a_2(1-a_2)\\end{bmatrix}.\\end{align*}$$\n","9. On Monday, we will focus on the first term to complete the gradient calculation using the computation graph.\n","\n","\n","---"]},{"cell_type":"markdown","metadata":{"id":"1YhZyZSh4kt5"},"source":["---\n","\n","CCE loss and its gradient\n","\n","$$\\begin{align*}L &= L\\left(\\mathbf{y},\\hat{\\mathbf{y}}\\right)=\\sum_{k=0}^2-y_k\\log\\left(\\hat{y}_k\\right)\\\\\\nabla_\\hat{\\mathbf{y}}(L) &=\\begin{bmatrix}\\nabla_{\\hat{y}_0}(L)\\\\\\nabla_{\\hat{y}_1}(L)\\\\\\nabla_{\\hat{y}_2}(L)\\end{bmatrix} = \\begin{bmatrix}-y_0/\\hat{y}_0\\\\-y_1/\\hat{y}_1\\\\-y_2/\\hat{y}_2\\end{bmatrix}.\\end{align*}$$\n","\n","\n","---"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"OWNtjgd14ofC"},"outputs":[],"source":["## Define the loss function and its gradient\n","def cce(y, yhat):\n","  return(-np.sum(y*np.log(yhat),axis=0))\n","\n","def cce_gradient(y, yhat):\n","  return(-y/yhat)\n","\n","# TensorFlow in-built function for categorical crossentropy loss\n","#cce = tf.keras.losses.CategoricalCrossentropy()"]},{"cell_type":"markdown","metadata":{"id":"rqGchtCX4eqM"},"source":["---\n","\n","Softmax activation layer class\n","$$\\begin{align*}\\text{forward:}\\ \\mathbf{a} &=\\text{softmax}(\\mathbf{z}),\\\\\\text{backward:}\\ \\nabla_\\mathbf{z}(L) &= \\nabla_{\\mathbf{z}}(\\mathbf{a})\\times\\nabla_{\\mathbf{a}}(L) = \\nabla_{\\mathbf{z}}(\\mathbf{a})\\times\\nabla_{\\hat{\\mathbf{y}}}(L)\\\\&=\\begin{bmatrix}a_0(1-a_0)&-a_1a_0&-a_2a_0\\\\-a_0a_1&a_1(1-a_1)&-a_2a_1\\\\-a_0a_2&-a_1a_2&a_2(1-a_2)\\end{bmatrix}\\begin{bmatrix}-y_0/\\hat{y}_0\\\\-y_1/\\hat{y}_1\\\\-y_2/\\hat{y}_2\\end{bmatrix}.\\end{align*}$$\n","\n","\n","---"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"4x1Xn3AbJlNy"},"outputs":[],"source":["## Softmax activation class\n","class Softmax(Layer):\n","  def forward(self, input):\n","    self.output = np.array(tf.nn.softmax(input))\n","\n","  def backward(self, output_gradient, learning_rate = None):\n","    return(np.dot((np.identity(np.size(self.output))-self.output.T) * self.output, output_gradient))"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"0UpPt9sCrNkQ"},"outputs":[],"source":["# Step-1: add the bias feature to all the samples\n","X = np.vstack([X, np.ones((1, num_samples))])"]},{"cell_type":"markdown","metadata":{"id":"dnYY5Nsx4Qtx"},"source":["---\n","\n","Calculate the gradient of the losstill the second to last term (that is, the gradient w.r.t. the input of the softmax activation layer) :\n","\n","\n","$$\\begin{align*}\\Rightarrow \\nabla_\\mathbf{W}(L) &= \\nabla_\\mathbf{W}(\\mathbf{z}) \\times\\nabla_\\mathbf{z}(\\mathbf{a})\\times\\nabla_\\mathbf{a}(L)\\\\&= \\underbrace{\\nabla_\\mathbf{W}(\\mathbf{z})}_\\text{first term} \\times\\underbrace{\\nabla_\\mathbf{z}(\\mathbf{a})}_\\text{second to last term}\\times\\underbrace{\\nabla_\\hat{\\mathbf{y}}(L)}_\\text{last term}.\\end{align*}$$\n","\n","---"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"LGIzrN-rPuI4"},"outputs":[{"name":"stdout","output_type":"stream","text":["0.09276563687585779\n","[[ 4.56  3.65  5.47  2.73  5.47  0.91]\n"," [ 4.56  3.65  5.47  2.73  5.47  0.91]\n"," [-0.44 -0.35 -0.53 -0.27 -0.53 -0.09]]\n","18.787388616005654\n","[[-5.00e+00 -9.00e+00 -3.00e+00 -4.00e+00 -7.00e+00 -1.00e+00]\n"," [ 3.47e-08  6.24e-08  2.08e-08  2.77e-08  4.85e-08  6.93e-09]\n"," [ 3.47e-08  6.24e-08  2.08e-08  2.77e-08  4.85e-08  6.93e-09]]\n","0.024964662214391625\n","[[ 2.93  4.88  5.85  2.93  2.93  0.98]\n"," [ 2.93  4.88  5.85  2.93  2.93  0.98]\n"," [-0.07 -0.12 -0.15 -0.07 -0.07 -0.02]]\n","7.7307672495905395\n","[[-9.00e+00 -9.00e+00 -9.00e+00 -3.00e+00 -5.00e+00 -1.00e+00]\n"," [ 3.95e-03  3.95e-03  3.95e-03  1.32e-03  2.20e-03  4.39e-04]\n"," [ 3.95e-03  3.95e-03  3.95e-03  1.32e-03  2.20e-03  4.39e-04]]\n","13.91003020821659\n","[[ 8.19e-06  6.37e-06  6.37e-06  8.19e-06  3.64e-06  9.10e-07]\n"," [-9.00e+00 -7.00e+00 -7.00e+00 -9.00e+00 -4.00e+00 -1.00e+00]\n"," [ 8.19e-06  6.37e-06  6.37e-06  8.19e-06  3.64e-06  9.10e-07]]\n","16.88552860065912\n","[[-4.00e+00 -4.00e+00 -9.00e+00 -4.00e+00 -8.00e+00 -1.00e+00]\n"," [ 1.86e-07  1.86e-07  4.18e-07  1.86e-07  3.71e-07  4.64e-08]\n"," [ 1.86e-07  1.86e-07  4.18e-07  1.86e-07  3.71e-07  4.64e-08]]\n","13.30788695865515\n","[[ 1.33e-05  1.33e-05  6.65e-06  1.16e-05  1.33e-05  1.66e-06]\n"," [-8.00e+00 -8.00e+00 -4.00e+00 -7.00e+00 -8.00e+00 -1.00e+00]\n"," [ 1.33e-05  1.33e-05  6.65e-06  1.16e-05  1.33e-05  1.66e-06]]\n","13.216881055973108\n","[[ 7.28e-06  1.46e-05  7.28e-06  1.46e-05  9.10e-06  1.82e-06]\n"," [-4.00e+00 -8.00e+00 -4.00e+00 -8.00e+00 -5.00e+00 -1.00e+00]\n"," [ 7.28e-06  1.46e-05  7.28e-06  1.46e-05  9.10e-06  1.82e-06]]\n","5.429831732373401\n","[[ 1.32e-02  2.63e-02  2.63e-02  2.63e-02  3.51e-02  4.38e-03]\n"," [-2.99e+00 -5.97e+00 -5.97e+00 -5.97e+00 -7.96e+00 -9.96e-01]\n"," [ 1.32e-02  2.63e-02  2.63e-02  2.63e-02  3.51e-02  4.38e-03]]\n","10.583802580031405\n","[[-8.00e+00 -6.00e+00 -3.00e+00 -3.00e+00 -9.00e+00 -1.00e+00]\n"," [ 2.03e-04  1.52e-04  7.60e-05  7.60e-05  2.28e-04  2.53e-05]\n"," [ 2.03e-04  1.52e-04  7.60e-05  7.60e-05  2.28e-04  2.53e-05]]\n"]}],"source":["## Train the 0-layer neural network using batch training with\n","## batch size = 1\n","\n","# Steps: run over each sample, calculate loss, gradient of loss,\n","# and update weights.\n","\n","\n","# Step-2: initialize the entries of the weights matrix randomly\n","W = np.random.normal(0, 1, (num_labels, num_features))\n","W = np.hstack([W, 0.01*np.ones((num_labels, 1))])\n","\n","# Step-3: create softmax layer object softmax\n","softmax = Softmax()\n","\n","# Step-4: run over each sample\n","for i in range(X.shape[1]):\n","  # Step-5: forward step\n","  # (a) Raw scores z = Wx\n","  z = np.dot(W, X[:,i])\n","  # (b) Softmax activation\n","  softmax.forward(z)\n","  # (c) Calculate cce loss for sample\n","  loss = cce(y[i, :], softmax.output)\n","  # (d) Print cce loss\n","  print(loss)\n","\n","  # Step-6: backward step\n","  # (a) Calculate the gradient of the sample loss w.r.t. input of the\n","  # softmax layer:\n","  grad = cce_gradient(y[i, :], softmax.output)\n","  grad = softmax.backward(output_gradient = grad)\n","  grad = grad.reshape(-1, 1) * X[:, i].reshape(-1, 1).T\n","  # (d) Print gradient\n","  print(grad)\n","  # Gradient descent step\n","  learning_rate = 1e-03\n","  W = W + learning_rate * (-grad)"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.17"}},"nbformat":4,"nbformat_minor":0}
