{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "**Load essential libraries**\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "CrW3wGfQN2KV"
      },
      "id": "CrW3wGfQN2KV"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "valued-lodging",
      "metadata": {
        "id": "valued-lodging"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "plt.style.use('dark_background')\n",
        "%matplotlib inline\n",
        "\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "**Check TensorFlow version**\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "CZmMj92lN6Yc"
      },
      "id": "CZmMj92lN6Yc"
    },
    {
      "cell_type": "code",
      "source": [
        "tf.__version__"
      ],
      "metadata": {
        "id": "B2VpALYvOBoG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "ba036a84-8366-4ec2-db9f-5efaf3dc4a49"
      },
      "id": "B2VpALYvOBoG",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.15.0'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "Answer the following questions inline using the documentation from:\n",
        " - Introduction to tensors https://www.tensorflow.org/guide/tensor\n",
        " - Introduction to variables https://www.tensorflow.org/guide/variable\n",
        " - Introduction to gradients and automatic differentiation https://www.tensorflow.org/guide/autodiff\n",
        "\n",
        " ---\n",
        "\n",
        "\n",
        "1. A scalar is a rank $\\underbrace{0/1/2}_\\text{choose one}$ tensor.\n",
        "2. *True/false*: a scalar has no axes.\n",
        "3. A matrix is a rank $\\underbrace{0/1/2}_\\text{choose one}$ tensor and has $\\underbrace{1/2}_\\text{choose one}$ axes.\n",
        "4. What does the function call $\\texttt{tf.reshape(A, [-1]}$ does for a given tensor $\\texttt{A}$?\n",
        "5. *True/false*: $\\texttt{tf.reshape()}$ can be used to swap axes of a tensor such as $\\texttt{(patients, timestamps, features)}$ to $\\texttt{(timestamps, patients, features)},$\n",
        "6. $\\texttt{tf.keras}$ uses $\\underline{\\qquad\\qquad\\qquad}$ to store model parameters.\n",
        "7. *True/false*: calling $\\texttt{assign}$ reuses a tensor's exisiting memory to assign the values.\n",
        "8. *True/false*:  creating a new tensor $\\texttt{b}$ based on the value of another tensor $\\texttt{a}$ as $\\texttt{b = tf.Variable(a)}$ will have the tensors allocated different memory.\n",
        "9. *True/false*: two tensor variables can have the same name.\n",
        "10. An example of a variable that would not need gradients is a $\\underline{\\qquad\\qquad\\qquad\\qquad}.$\n",
        "11. Tensor variables are typically placed in $\\underbrace{\\text{CPU/GPU}}_{\\text{choose one}}.$\n",
        "12. *True/false*: a tensor variable is trainable by default.\n",
        "13. A gradient tape tape will automatically watch a $\\underbrace{\\texttt{tf.Variable/tf.constant}}_\\text{choose one}$ but not a $\\underbrace{\\texttt{tf.Variable/tf.constant}}_\\text{choose one}.$\n",
        "14. What attribute can be used to calculate a layer's gradient w.r.t. all its trainable variables?\n",
        "15. The option $\\texttt{persistent = True}$ for a gradient tape $\\underbrace{\\texttt{stores/discards}}_\\text{choose one}$ all intermediate results during the forward pass.\n",
        "16. Executing the statement $\\texttt{print(type(x).__name__)}$ when $\\texttt{x}$ is a constant and when $\\texttt{x}$ is a variable results in what?\n",
        "17. Which one among $\\texttt{tf.Tensor}$ and $\\texttt{tf.Variable}$ is immutable? Which one has no state but only value? Which one has a state which is actually its value?\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "V6MJcE8mQuxG"
      },
      "id": "V6MJcE8mQuxG"
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "Consider a 1-layer neural network for a sample with 3 features: heart rate, blood pressure, and temperature and 2 possible output categories: diabetic and non-diabetic.\n",
        "\n",
        "An individual who is diabetic has heart rate = 76 BPM, BP = 120 mm Hg, and temperature = 37.5 $^\\circ\\text{C}.$\n",
        "\n",
        "Here is the forward propagation: $$\\mathbf{x}\\longrightarrow\\mathbf{x}_B=\\begin{bmatrix}\\mathbf{x}\\\\1\\end{bmatrix}\\longrightarrow\\mathbf{z}^{[1]} = \\mathbf{W}^{[1]}\\mathbf{x}_B\\longrightarrow\\underbrace{\\mathbf{a}^{[1]}}_{=\\hat{\\mathbf{y}}} = \\text{softmax}\\left(\\mathbf{a}^{[1]}\\right)\\longrightarrow L = \\sum_{k=0}^1-y_k\\log\\left(\\hat{y}_k\\right).$$\n",
        "\n",
        "*   Fill in the missing entries of the code below to calculate the gradients there-in.\n",
        "* Explain why some gradient shapes do not seem to match with the usual $\\text{input shape}\\times\\text{output shape}$ rule for gradient shapes using the documentation on [Gradients of non-scalar target](https://www.tensorflow.org/guide/autodiff#gradients_of_non-scalar_targets) as resource.\n",
        "* Try $\\texttt{persistent = true}$ and $\\texttt{persistent = false}$ in the gradient tape and observe what happens in each case.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "UCTm9rVXCosL"
      },
      "id": "UCTm9rVXCosL"
    },
    {
      "cell_type": "code",
      "source": [
        "x = tf.constant([?, ?, ?])\n",
        "y = tf.constant([?, ?])\n",
        "xB = tf.concat([?, 1.0*tf.ones(1)], axis = 0)\n",
        "W = tf.?(0.01*(tf.random.normal((?, ?))))\n",
        "\n",
        "with tf.GradientTape(persistent = ?) as g:\n",
        "  z = tf.linalg.matvec(?, ?)\n",
        "  a = tf.nn.softmax(?)\n",
        "  yhat = ?\n",
        "  L = tf.reduce_?(-?*tf.math.log(?))\n",
        "\n",
        "print('Loss = %f'%(L))\n",
        "print('Gradient of L w.r.t. yhat')\n",
        "gradL_yhat = g.gradient(?, ?)\n",
        "print(gradL_yhat)\n",
        "print('-----')\n",
        "print('Gradient of a w.r.t. z')\n",
        "grada_z = g.gradient(?, ?)\n",
        "print(grada_z)\n",
        "print('-----')\n",
        "print('Gradient of z w.r.t. W')\n",
        "gradz_W = g.gradient(?, ?)\n",
        "print(gradz_W)\n",
        "print('-----')\n",
        "print('Gradient of L w.r.t. W')\n",
        "gradL_W = g.gradient(?, ?)\n",
        "print(gradL_W)\n",
        "\n",
        "# Delete gradient tape and release memory\n",
        "del g"
      ],
      "metadata": {
        "id": "5ZWJEqidgohL"
      },
      "id": "5ZWJEqidgohL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "Recalculate gradients pen-and-paper-way with the same weights from above using numpy. Compare the gradient results here with the ones that you had from the previous cell. Why are some gradients different? In both approaches (in this cell and in the one above), is the gradient of interest $\\nabla_{\\mathbf{W}^{[1]}}(L)$ the same? Note that this is the only gradient we need to update the weights matrix $\\mathbf{W}^{[1]}.$\n",
        "\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "TyzN0acKT0v_"
      },
      "id": "TyzN0acKT0v_"
    },
    {
      "cell_type": "code",
      "source": [
        "xB_np = xB.numpy().reshape(-1, 1) # bias-feature added sample vector for numpy\n",
        "y = np.array([?, ?]) #\n",
        "z = np.dot(W.numpy(), ?) # note we use the same weights from the previous cell here\n",
        "a = tf.nn.softmax(?, axis = 0).numpy().flatten()\n",
        "yhat = ?\n",
        "L = tf.reduce_sum(-?*?)\n",
        "\n",
        "print('Loss = %f'%(L))\n",
        "print('-----')\n",
        "print('Gradient of L w.r.t. yhat')\n",
        "gradL_yhat = (? / ?)\n",
        "print(gradL_yhat)\n",
        "print('-----')\n",
        "print('Gradient of a w.r.t. z')\n",
        "grada_z = (np.identity(np.size(?))-?.reshape(-1,?).T) * a.reshape(?, ?)\n",
        "print(grada_z)\n",
        "print('-----')\n",
        "print('Gradient of z w.r.t. W')\n",
        "gradz_W = np.zeros((?.shape[0], W.shape[1], ?.shape[0]))\n",
        "gradz_W[range(?), :, range(?)] = ?.flatten()\n",
        "print(gradz_W)\n",
        "print('-----')\n",
        "print('Gradient of L w.r.t. W')\n",
        "gradL_W = np.dot(?, np.dot(?, ?.reshape(-1, 1))).squeeze()\n",
        "print(gradL_W)"
      ],
      "metadata": {
        "id": "Hi-iWWZ4T48I"
      },
      "id": "Hi-iWWZ4T48I",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "For each activation function below,\n",
        "\n",
        "1.   Sigmoid $\\sigma(z)$\n",
        "2.   Hyperbolic tangent $\\tanh(z)$\n",
        "3.   Rectified Linear Unit $\\text{ReLU}(z)$\n",
        "4.   Leaky rectified linear unit $\\text{LReLU}(z)$\n",
        "\n",
        "*   plot the activation and its gradient in the same figure for raw score values $z$ ranging between $-10$ and $10$;\n",
        "*   comment on whether the backward flowing gradient on the input side of the activation layer will have a smaller or bigger magnitude compared to the backward flowing gradient on the output side of the activation layer. Recall that what connects these two gradients is the local gradient of the activation layer which you may have just plotted.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "gCXq273SvCB8"
      },
      "id": "gCXq273SvCB8"
    },
    {
      "cell_type": "code",
      "source": [
        "z = tf.linspace(?, ?, 129) # A tf.Tensor, not a tf.Variable\n",
        "\n",
        "with tf.GradientTape(persistent = ?) as g:\n",
        "    g.?(?)\n",
        "    a_sigmoid = tf.math.?(z)\n",
        "    a_tanh = tf.math.?(z)\n",
        "    a_ReLU = z * tf.cast((z > ?), tf.float64)\n",
        "    a_LReLU = ? + 0.01*z*tf.cast((z ? 0), tf.float64)\n",
        "\n",
        "grada_sigmoid_z = g.gradient(?, ?)\n",
        "grada_tanh_z = g.gradient(?, ?)\n",
        "grada_ReLU_z = g.gradient(?, ?)\n",
        "grada_LReLU_z = g.gradient(?, ?)\n",
        "\n",
        "fig, axs = plt.subplots(2, 2, figsize = (8, 8))\n",
        "axs[0, 0].plot(?, ?, label = 'activated score')\n",
        "axs[0, 0].plot(?, ?, label='gradient of activated score')\n",
        "axs[0, 0].legend(loc = 'upper left')\n",
        "axs[0, 0].set_xlabel('z')\n",
        "axs[0, 0].set_title('Sigmoid activation and gradient');\n",
        "\n",
        "axs[?, ?].plot(?, ?, label = 'activated score')\n",
        "axs[?, ?].plot(?, ?, label='gradient of activated score')\n",
        "axs[?, ?].legend(loc = 'upper left')\n",
        "axs[?, ?].set_?('z')\n",
        "axs[?, ?].set_?('Tanh activation and gradient');\n",
        "\n",
        "axs[?, ?].plot(?, ?, label = 'activated score')\n",
        "axs[?, ?].plot(?, ?, label='gradient of activated score')\n",
        "axs[?, ?].legend(loc = 'upper left')\n",
        "axs[?, ?].?('z')\n",
        "axs[?, ?].?('ReLU activation and gradient');\n",
        "\n",
        "axs[?, ?].plot(?, ?, label = 'activated score')\n",
        "axs[?, ?].plot(?, ?, label='gradient of activated score')\n",
        "axs[?, ?].?(loc = 'upper left')\n",
        "axs[?, ?].?('z')\n",
        "axs[?, ?].?('Leaky ReLU activation and gradient');"
      ],
      "metadata": {
        "id": "AFQir0Hwvoyd"
      },
      "id": "AFQir0Hwvoyd",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}