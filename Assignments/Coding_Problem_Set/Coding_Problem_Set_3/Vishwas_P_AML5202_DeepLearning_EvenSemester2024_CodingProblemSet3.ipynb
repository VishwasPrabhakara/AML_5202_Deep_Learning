{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "5dEgRpy3952M"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From c:\\Users\\vp140\\.conda\\envs\\pycaretenv\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "## Load libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import sys\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.cm as cm\n",
        "from keras.datasets import mnist\n",
        "plt.style.use('dark_background')\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "G9W_1_v_6yq7"
      },
      "outputs": [],
      "source": [
        "np.set_printoptions(precision=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "4T7eUtw7Mh0z"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Q1e2N5S8MlCU"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'2.15.0'"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tf.__version__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "16BpVeIWIOks"
      },
      "source": [
        "---\n",
        "\n",
        "Load MNIST Data\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "E5kaKFKSIQgu"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MNIST set\n",
            "---------------------\n",
            "Number of training samples = 60000\n",
            "Number of features = 784\n",
            "Number of output labels = 10\n"
          ]
        }
      ],
      "source": [
        "## Load MNIST data\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "X_train = X_train.transpose(1, 2, 0)\n",
        "X_test = X_test.transpose(1, 2, 0)\n",
        "X_train = X_train.reshape(X_train.shape[0]*X_train.shape[1], X_train.shape[2])\n",
        "X_test = X_test.reshape(X_test.shape[0]*X_test.shape[1], X_test.shape[2])\n",
        "\n",
        "num_labels = len(np.unique(y_train))\n",
        "num_features = X_train.shape[0]\n",
        "num_samples = X_train.shape[1]\n",
        "\n",
        "# One-hot encode class labels\n",
        "Y_train = tf.keras.utils.to_categorical(y_train).T\n",
        "Y_test = tf.keras.utils.to_categorical(y_test).T\n",
        "\n",
        "\n",
        "# Normalize the samples (images)\n",
        "xmax = np.amax(X_train)\n",
        "xmin = np.amin(X_train)\n",
        "X_train = (X_train - xmin) / (xmax - xmin) # all train features turn into a number between 0 and 1\n",
        "X_test = (X_test - xmin)/(xmax - xmin)\n",
        "\n",
        "print('MNIST set')\n",
        "print('---------------------')\n",
        "print('Number of training samples = %d'%(num_samples))\n",
        "print('Number of features = %d'%(num_features))\n",
        "print('Number of output labels = %d'%(num_labels))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IrXipxwrJ0_8"
      },
      "source": [
        "---\n",
        "\n",
        "A generic layer class with forward and backward methods\n",
        "\n",
        "----"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "N4pKUhCyMrWm"
      },
      "outputs": [],
      "source": [
        "class Layer:\n",
        "  def __init__(self):\n",
        "    self.input = None\n",
        "    self.output = None\n",
        "\n",
        "  def forward(self, input):\n",
        "    pass\n",
        "\n",
        "  def backward(self, output_gradient, learning_rate):\n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UMt81Faf9-bf"
      },
      "source": [
        "---\n",
        "\n",
        "CCE loss and its gradient\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "hdXSGW2s7zKd"
      },
      "outputs": [],
      "source": [
        "## Define the loss function and its gradient\n",
        "def cce(Y, Yhat):\n",
        "  return(np.mean(np.sum(-Y*np.log(Yhat), axis = 0)))\n",
        "  #TensorFlow in-built function for categorical crossentropy loss\n",
        "  #cce = tf.keras.losses.CategoricalCrossentropy()\n",
        "  #return(cce(Y, Yhat).numpy())\n",
        "\n",
        "def cce_gradient(Y, Yhat):\n",
        "  return(-Y/Yhat)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jmcNJTjS-BaW"
      },
      "source": [
        "---\n",
        "\n",
        "Generic activation layer class\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "C21FcWIEwGCN"
      },
      "outputs": [],
      "source": [
        "class Activation(Layer):\n",
        "    def __init__(self, activation, activation_gradient):\n",
        "        self.activation = activation\n",
        "        self.activation_gradient = activation_gradient\n",
        "\n",
        "    def forward(self, input):\n",
        "        self.input = input\n",
        "        self.output = self.activation(self.input)\n",
        "        return(self.output)\n",
        "\n",
        "    def backward(self, output_gradient, learning_rate = None):\n",
        "        return(output_gradient[:-1, :] * self.activation_gradient(self.input))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JheGWSoKxYWu"
      },
      "source": [
        "---\n",
        "\n",
        "Specific activation layer classes\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "PQ5ybz_Yxbef"
      },
      "outputs": [],
      "source": [
        "class Sigmoid(Activation):\n",
        "    def __init__(self):\n",
        "        def sigmoid(z):\n",
        "            return 1 / (1 + np.exp(-z))\n",
        "\n",
        "        def sigmoid_gradient(z):\n",
        "            a = sigmoid(z)\n",
        "            return a * (1 - a)\n",
        "\n",
        "        super().__init__(sigmoid, sigmoid_gradient)\n",
        "\n",
        "class Tanh(Activation):\n",
        "    def __init__(self):\n",
        "        def tanh(z):\n",
        "            return np.tanh(z)\n",
        "\n",
        "        def tanh_gradient(z):\n",
        "            return 1 - np.tanh(z) ** 2\n",
        "\n",
        "        super().__init__(tanh, tanh_gradient)\n",
        "\n",
        "class ReLU(Activation):\n",
        "    def __init__(self):\n",
        "        def relu(z):\n",
        "            return z * (z > 0)\n",
        "\n",
        "        def relu_gradient(z):\n",
        "            return 1. * (z > 0)\n",
        "\n",
        "        super().__init__(relu, relu_gradient)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LGdr2m2R-It8"
      },
      "source": [
        "---\n",
        "\n",
        "Softmax activation layer class\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "4x1Xn3AbJlNy"
      },
      "outputs": [],
      "source": [
        "## Softmax activation layer class\n",
        "class Softmax(Layer):\n",
        "  def forward(self, input):\n",
        "    self.output = tf.nn.softmax(input, axis = 0).numpy()\n",
        "\n",
        "  def backward(self, output_gradient, learning_rate = None):\n",
        "    ## Following is the inefficient way of calculating the backward gradient\n",
        "    softmax_gradient = np.empty((self.output.shape[0], output_gradient.shape[1]), dtype = np.float64)\n",
        "    for b in range(softmax_gradient.shape[1]):\n",
        "      softmax_gradient[:, b] = np.dot((np.identity(self.output.shape[0])-np.atleast_2d(self.output[:, b])) * np.atleast_2d(self.output[:, b]).T, output_gradient[:, b])\n",
        "\n",
        "    # Return gradient w.r.t. input for backward propagation\n",
        "    return(softmax_gradient)\n",
        "\n",
        "    ## Following is the efficient of calculating the backward gradient\n",
        "    #T = np.transpose(np.identity(self.output.shape[0]) - np.atleast_2d(self.output).T[:, np.newaxis, :], (1, 2, 0)) * np.atleast_2d(self.output)\n",
        "    #return(np.einsum('ijk, ik -> jk', T, output_gradient))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UKnqi7rf-MBn"
      },
      "source": [
        "---\n",
        "\n",
        "Dense layer class\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "8ctXhZYCTmHK"
      },
      "outputs": [],
      "source": [
        "## Dense layer class\n",
        "class Dense(Layer):\n",
        "    def __init__(self, input_size, output_size):\n",
        "        self.weights = 0.01*np.random.randn(output_size, input_size+1) # bias trick\n",
        "        self.weights[:, -1] = 0.01 # set all bias values to the same nonzero constant\n",
        "\n",
        "    def forward(self, input):\n",
        "        self.input = np.vstack([input, np.ones((1, input.shape[1]))]) # bias trick\n",
        "        self.output= np.dot(self.weights, self.input)\n",
        "\n",
        "    def backward(self, output_gradient, learning_rate):\n",
        "        ## Following is the inefficient way of calculating the gradient w.r.t. weights\n",
        "        weights_gradient = np.zeros((self.output.shape[0], self.input.shape[0]), dtype = np.float64)\n",
        "        for b in range(output_gradient.shape[1]):\n",
        "          weights_gradient += np.dot(output_gradient[:, b].reshape(-1, 1), self.input[:, b].reshape(-1, 1).T)\n",
        "        weights_gradient = (1/output_gradient.shape[1])*weights_gradient\n",
        "\n",
        "        ## Following is the efficient way of calculating the weightsgradient\n",
        "        #weights_gradient = (1/output_gradient.shape[1])*np.dot(np.atleast_2d(output_gradient), np.atleast_2d(self.input).T)\n",
        "\n",
        "        # Gradient w.r.t. the input\n",
        "        input_gradient = np.dot(self.weights.T, output_gradient)\n",
        "\n",
        "        # Update weights using gradient descent step\n",
        "        self.weights = self.weights + learning_rate * (-weights_gradient)\n",
        "\n",
        "        # Return gradient w.r.t. input for backward propagation\n",
        "        return(input_gradient)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2W1howeOJegI"
      },
      "source": [
        "---\n",
        "\n",
        "Function to generate sample indices for batch processing according to batch size\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "MHyjEf22IRpc"
      },
      "outputs": [],
      "source": [
        "## Function to generate sample indices for batch processing according to batch size\n",
        "def generate_batch_indices(num_samples, batch_size):\n",
        "  # Reorder sample indices\n",
        "  reordered_sample_indices = np.random.choice(num_samples, num_samples, replace = False)\n",
        "  # Generate batch indices for batch processing\n",
        "  batch_indices = np.split(reordered_sample_indices, np.arange(batch_size, len(reordered_sample_indices), batch_size))\n",
        "  return(batch_indices)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fI_Gms9fJqbs"
      },
      "source": [
        "---\n",
        "\n",
        "Train the 1-hidden layer neural network (128 nodes) using batch training with batch size = 100\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "LGIzrN-rPuI4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1: loss = 2.298796\n",
            "Epoch 2: loss = 2.290666\n",
            "Epoch 3: loss = 2.278866\n",
            "Epoch 4: loss = 2.260469\n",
            "Epoch 5: loss = 2.231436\n",
            "Epoch 6: loss = 2.186450\n",
            "Epoch 7: loss = 2.119613\n",
            "Epoch 8: loss = 2.026019\n",
            "Epoch 9: loss = 1.904079\n",
            "Epoch 10: loss = 1.758383\n",
            "Epoch 11: loss = 1.600563\n",
            "Epoch 12: loss = 1.445367\n",
            "Epoch 13: loss = 1.304079\n",
            "Epoch 14: loss = 1.181739\n",
            "Epoch 15: loss = 1.078536\n",
            "Epoch 16: loss = 0.992365\n",
            "Epoch 17: loss = 0.920443\n",
            "Epoch 18: loss = 0.860121\n",
            "Epoch 19: loss = 0.809190\n",
            "Epoch 20: loss = 0.765806\n",
            "Epoch 21: loss = 0.728518\n",
            "Epoch 22: loss = 0.696218\n",
            "Epoch 23: loss = 0.667946\n",
            "Epoch 24: loss = 0.643071\n",
            "Epoch 25: loss = 0.620950\n",
            "Epoch 26: loss = 0.601229\n",
            "Epoch 27: loss = 0.583466\n",
            "Epoch 28: loss = 0.567419\n",
            "Epoch 29: loss = 0.552848\n",
            "Epoch 30: loss = 0.539558\n",
            "Epoch 31: loss = 0.527372\n",
            "Epoch 32: loss = 0.516182\n",
            "Epoch 33: loss = 0.505854\n",
            "Epoch 34: loss = 0.496290\n",
            "Epoch 35: loss = 0.487452\n",
            "Epoch 36: loss = 0.479217\n",
            "Epoch 37: loss = 0.471535\n",
            "Epoch 38: loss = 0.464388\n",
            "Epoch 39: loss = 0.457676\n",
            "Epoch 40: loss = 0.451380\n",
            "Epoch 41: loss = 0.445471\n",
            "Epoch 42: loss = 0.439917\n",
            "Epoch 43: loss = 0.434687\n",
            "Epoch 44: loss = 0.429714\n",
            "Epoch 45: loss = 0.425027\n",
            "Epoch 46: loss = 0.420613\n",
            "Epoch 47: loss = 0.416385\n",
            "Epoch 48: loss = 0.412408\n",
            "Epoch 49: loss = 0.408594\n",
            "Epoch 50: loss = 0.404990\n",
            "Epoch 51: loss = 0.401515\n",
            "Epoch 52: loss = 0.398215\n",
            "Epoch 53: loss = 0.395049\n",
            "Epoch 54: loss = 0.392032\n",
            "Epoch 55: loss = 0.389137\n",
            "Epoch 56: loss = 0.386319\n",
            "Epoch 57: loss = 0.383674\n",
            "Epoch 58: loss = 0.381091\n",
            "Epoch 59: loss = 0.378608\n",
            "Epoch 60: loss = 0.376200\n",
            "Epoch 61: loss = 0.373904\n",
            "Epoch 62: loss = 0.371687\n",
            "Epoch 63: loss = 0.369507\n",
            "Epoch 64: loss = 0.367431\n",
            "Epoch 65: loss = 0.365408\n",
            "Epoch 66: loss = 0.363457\n",
            "Epoch 67: loss = 0.361545\n",
            "Epoch 68: loss = 0.359706\n",
            "Epoch 69: loss = 0.357909\n",
            "Epoch 70: loss = 0.356151\n",
            "Epoch 71: loss = 0.354478\n",
            "Epoch 72: loss = 0.352796\n",
            "Epoch 73: loss = 0.351214\n",
            "Epoch 74: loss = 0.349646\n",
            "Epoch 75: loss = 0.348095\n",
            "Epoch 76: loss = 0.346602\n",
            "Epoch 77: loss = 0.345152\n",
            "Epoch 78: loss = 0.343730\n",
            "Epoch 79: loss = 0.342323\n",
            "Epoch 80: loss = 0.340955\n",
            "Epoch 81: loss = 0.339630\n",
            "Epoch 82: loss = 0.338322\n",
            "Epoch 83: loss = 0.337028\n",
            "Epoch 84: loss = 0.335787\n",
            "Epoch 85: loss = 0.334531\n",
            "Epoch 86: loss = 0.333314\n",
            "Epoch 87: loss = 0.332135\n",
            "Epoch 88: loss = 0.330967\n",
            "Epoch 89: loss = 0.329803\n",
            "Epoch 90: loss = 0.328660\n",
            "Epoch 91: loss = 0.327571\n",
            "Epoch 92: loss = 0.326490\n",
            "Epoch 93: loss = 0.325395\n",
            "Epoch 94: loss = 0.324328\n",
            "Epoch 95: loss = 0.323273\n",
            "Epoch 96: loss = 0.322270\n",
            "Epoch 97: loss = 0.321230\n",
            "Epoch 98: loss = 0.320227\n",
            "Epoch 99: loss = 0.319253\n",
            "Epoch 100: loss = 0.318266\n",
            "Epoch 101: loss = 0.317312\n",
            "Epoch 102: loss = 0.316366\n",
            "Epoch 103: loss = 0.315414\n",
            "Epoch 104: loss = 0.314482\n",
            "Epoch 105: loss = 0.313578\n",
            "Epoch 106: loss = 0.312667\n",
            "Epoch 107: loss = 0.311766\n",
            "Epoch 108: loss = 0.310879\n",
            "Epoch 109: loss = 0.310000\n",
            "Epoch 110: loss = 0.309133\n",
            "Epoch 111: loss = 0.308274\n",
            "Epoch 112: loss = 0.307422\n",
            "Epoch 113: loss = 0.306571\n",
            "Epoch 114: loss = 0.305750\n",
            "Epoch 115: loss = 0.304939\n",
            "Epoch 116: loss = 0.304109\n",
            "Epoch 117: loss = 0.303307\n",
            "Epoch 118: loss = 0.302498\n",
            "Epoch 119: loss = 0.301716\n",
            "Epoch 120: loss = 0.300912\n",
            "Epoch 121: loss = 0.300138\n",
            "Epoch 122: loss = 0.299354\n",
            "Epoch 123: loss = 0.298585\n",
            "Epoch 124: loss = 0.297826\n",
            "Epoch 125: loss = 0.297065\n",
            "Epoch 126: loss = 0.296320\n",
            "Epoch 127: loss = 0.295556\n",
            "Epoch 128: loss = 0.294837\n",
            "Epoch 129: loss = 0.294093\n",
            "Epoch 130: loss = 0.293351\n",
            "Epoch 131: loss = 0.292623\n",
            "Epoch 132: loss = 0.291910\n",
            "Epoch 133: loss = 0.291186\n",
            "Epoch 134: loss = 0.290481\n",
            "Epoch 135: loss = 0.289774\n",
            "Epoch 136: loss = 0.289066\n",
            "Epoch 137: loss = 0.288379\n",
            "Epoch 138: loss = 0.287672\n",
            "Epoch 139: loss = 0.286978\n",
            "Epoch 140: loss = 0.286311\n",
            "Epoch 141: loss = 0.285612\n",
            "Epoch 142: loss = 0.284897\n",
            "Epoch 143: loss = 0.284253\n",
            "Epoch 144: loss = 0.283598\n",
            "Epoch 145: loss = 0.282926\n",
            "Epoch 146: loss = 0.282264\n",
            "Epoch 147: loss = 0.281599\n",
            "Epoch 148: loss = 0.280949\n",
            "Epoch 149: loss = 0.280286\n",
            "Epoch 150: loss = 0.279642\n",
            "Epoch 151: loss = 0.278997\n",
            "Epoch 152: loss = 0.278335\n",
            "Epoch 153: loss = 0.277712\n",
            "Epoch 154: loss = 0.277075\n",
            "Epoch 155: loss = 0.276441\n",
            "Epoch 156: loss = 0.275797\n",
            "Epoch 157: loss = 0.275193\n",
            "Epoch 158: loss = 0.274570\n",
            "Epoch 159: loss = 0.273931\n",
            "Epoch 160: loss = 0.273330\n",
            "Epoch 161: loss = 0.272713\n",
            "Epoch 162: loss = 0.272083\n",
            "Epoch 163: loss = 0.271514\n",
            "Epoch 164: loss = 0.270899\n",
            "Epoch 165: loss = 0.270297\n",
            "Epoch 166: loss = 0.269698\n",
            "Epoch 167: loss = 0.269099\n",
            "Epoch 168: loss = 0.268503\n",
            "Epoch 169: loss = 0.267937\n",
            "Epoch 170: loss = 0.267346\n",
            "Epoch 171: loss = 0.266752\n",
            "Epoch 172: loss = 0.266177\n",
            "Epoch 173: loss = 0.265606\n",
            "Epoch 174: loss = 0.265012\n",
            "Epoch 175: loss = 0.264461\n",
            "Epoch 176: loss = 0.263890\n",
            "Epoch 177: loss = 0.263322\n",
            "Epoch 178: loss = 0.262761\n",
            "Epoch 179: loss = 0.262185\n",
            "Epoch 180: loss = 0.261618\n",
            "Epoch 181: loss = 0.261066\n",
            "Epoch 182: loss = 0.260530\n",
            "Epoch 183: loss = 0.259976\n",
            "Epoch 184: loss = 0.259423\n",
            "Epoch 185: loss = 0.258878\n",
            "Epoch 186: loss = 0.258333\n",
            "Epoch 187: loss = 0.257789\n",
            "Epoch 188: loss = 0.257250\n",
            "Epoch 189: loss = 0.256730\n",
            "Epoch 190: loss = 0.256184\n",
            "Epoch 191: loss = 0.255640\n",
            "Epoch 192: loss = 0.255129\n",
            "Epoch 193: loss = 0.254594\n",
            "Epoch 194: loss = 0.254066\n",
            "Epoch 195: loss = 0.253542\n",
            "Epoch 196: loss = 0.253010\n",
            "Epoch 197: loss = 0.252497\n",
            "Epoch 198: loss = 0.251966\n",
            "Epoch 199: loss = 0.251462\n",
            "Epoch 200: loss = 0.250942\n"
          ]
        }
      ],
      "source": [
        "## Train the 1-hidden layer neural network (128 nodes)\n",
        "## using batch training with batch size = 100\n",
        "learning_rate = 1e-3 # learning rate\n",
        "batch_size = 100 # batch size\n",
        "nepochs = 200 # number of epochs\n",
        "loss_epoch = np.empty(nepochs, dtype = np.float64) # create empty array to store losses over each epoch\n",
        "\n",
        "# Neural network architecture\n",
        "\n",
        "dlayer1 = Dense(num_features, 128) # define dense layer 1\n",
        "alayer1 = ReLU() # ReLU activation layer 1\n",
        "dlayer2 = Dense(128, num_labels) # define dense layer 2\n",
        "softmax = Softmax() # define softmax activation layer\n",
        "\n",
        "# Steps: run over each sample in the batch, calculate loss, gradient of loss,\n",
        "# and update weights.\n",
        "\n",
        "epoch = 0\n",
        "while epoch < nepochs:\n",
        "  batch_indices = generate_batch_indices(num_samples, batch_size)\n",
        "  loss = 0\n",
        "  for b in range(len(batch_indices)):\n",
        "    dlayer1.forward(X_train[:, batch_indices[b]]) # forward prop dense layer 1 with batch feature added\n",
        "    alayer1.forward(dlayer1.output) # forward prop activation layer 1\n",
        "    dlayer2.forward(alayer1.output) # forward prop dense layer 2\n",
        "    softmax.forward(dlayer2.output) # Softmax activate\n",
        "    loss += cce(Y_train[:, batch_indices[b]], softmax.output) # calculate loss\n",
        "    # Backward prop starts here\n",
        "    grad = cce_gradient(Y_train[:, batch_indices[b]], softmax.output)\n",
        "    grad = softmax.backward(grad)\n",
        "    grad = dlayer2.backward(grad, learning_rate)\n",
        "    grad = alayer1.backward(grad)\n",
        "    grad = dlayer1.backward(grad, learning_rate)\n",
        "  loss_epoch[epoch] = loss/len(batch_indices)\n",
        "  print('Epoch %d: loss = %f'%(epoch+1, loss_epoch[epoch]))\n",
        "  epoch = epoch + 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EhXEFASk-Tkv"
      },
      "source": [
        "---\n",
        "\n",
        "Plot training loss vs. epoch\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "Iv3k23SlCqGf"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Plot training loss as a function of epoch:\n",
        "plt.plot(loss_epoch)\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss value')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TaLoOOWK-WBj"
      },
      "source": [
        "---\n",
        "\n",
        "Test performance on test data\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "d7AEbmpcKcPY"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[7 2 1 ... 4 5 6]\n",
            "[7 2 1 ... 4 5 6]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "0.9305"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dlayer1.forward(X_test)\n",
        "alayer1.forward(dlayer1.output)\n",
        "dlayer2.forward(alayer1.output)\n",
        "softmax.forward(dlayer2.output)\n",
        "ypred = np.argmax(softmax.output.T, axis = 1)\n",
        "print(ypred)\n",
        "ytrue = np.argmax(Y_test.T, axis = 1)\n",
        "print(ytrue)\n",
        "np.mean(ytrue == ypred)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.17"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
