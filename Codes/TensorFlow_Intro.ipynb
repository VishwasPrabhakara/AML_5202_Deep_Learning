{"cells":[{"cell_type":"markdown","source":["From https://www.tensorflow.org/guide/basics\n","\n","TensorFlow (TF) is an end-to-end platform for machine learning. It supports the following:\n","\n","1. Multidimensional-array based numeric computation (similar to NumPy.)\n","2. GPU and distributed processing\n","3. Automatic differentiation\n","4. Model construction, training, and export\n","5. And more\n","\n","---\n","\n","Some important points about TF.\n","\n","* TensorFlow was developed by the Google Brain team for internal use at Google. Then, it was made open source in November 2015.\n","* Keras is an API (application programming interface) for deep learning calculations. API means that Keras defines a specific interface to write codes.\n","* Keras was written in Python.\n","* Keras has multiple backends (libraries that are\n","responsible for doing the actual calculations): TensorFlow,\n","CNTK, Theano.\n","* Keras is focused on easy and fast prototyping, through\n","user friendliness, modularity, and extensibility.\n","* Although TF can be used as a backend for Keras, it is\n","recommended to use tf.keras, which is the implementation\n","of Keras in TF.\n","\n","\n","\n"],"metadata":{"id":"ibsqZFNDOtcL"},"id":"ibsqZFNDOtcL"},{"cell_type":"markdown","source":["---\n","\n","**Load essential libraries**\n","\n","---"],"metadata":{"id":"CrW3wGfQN2KV"},"id":"CrW3wGfQN2KV"},{"cell_type":"code","execution_count":null,"id":"valued-lodging","metadata":{"id":"valued-lodging"},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","plt.style.use('seaborn-whitegrid')\n","%matplotlib inline\n","\n","import tensorflow as tf"]},{"cell_type":"markdown","source":["---\n","\n","**Check TensorFlow version**\n","\n","---"],"metadata":{"id":"CZmMj92lN6Yc"},"id":"CZmMj92lN6Yc"},{"cell_type":"code","source":["tf.__version__"],"metadata":{"id":"B2VpALYvOBoG"},"id":"B2VpALYvOBoG","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---\n","Introduction to tensors from https://www.tensorflow.org/guide/tensor\n","\n","---"],"metadata":{"id":"V6MJcE8mQuxG"},"id":"V6MJcE8mQuxG"},{"cell_type":"code","source":["# Some code examples here"],"metadata":{"id":"4CbC5yVsQyth"},"id":"4CbC5yVsQyth","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---\n","\n","Introduction to variables from https://www.tensorflow.org/guide/variable\n","\n","---"],"metadata":{"id":"CHCPJZG6QzKr"},"id":"CHCPJZG6QzKr"},{"cell_type":"code","source":["# Some code examples here"],"metadata":{"id":"gGjQqL8gQ8rk"},"id":"gGjQqL8gQ8rk","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---\n","\n","Automatic differentiation using TF (https://www.tensorflow.org/guide/autodiff)\n","\n","Example: calculate the sensitivity of $L(w) = 4w+w^3$ w.r.t. the input $w$ at $w=1.$\n","\n","Sensitivity $\\nabla_wL = 4+3w^2,$ which at $w=1$ is equal to $4+3\\times1^2=7.$\n","\n","---"],"metadata":{"id":"6U1TokFxQ_w2"},"id":"6U1TokFxQ_w2"},{"cell_type":"code","execution_count":null,"id":"banner-guatemala","metadata":{"id":"banner-guatemala"},"outputs":[],"source":["w = tf.Variable(1.0)\n","\n","with tf.GradientTape() as g:\n","    L = 4*w + w**3\n","\n","gradL_w = g.gradient(L, w)\n","print('gradient of L w.r.t. w at w = 2 is', gradL_w)"]},{"cell_type":"markdown","id":"better-restoration","metadata":{"id":"better-restoration"},"source":["---\n","\n","Example: calculate the sensitivity of $L(w_1,w_2) = w_1+w_2^2$ w.r.t. the inputs $w_1, w_2$ at $w_1=1, w_2=2.$\n","\n","Setting $\\mathbf{w} = \\begin{bmatrix}w_1\\\\w_2\\end{bmatrix},$ sensitivity $\\nabla_\\mathbf{w}L= \\begin{bmatrix}\\nabla_{w_1}(w_1+w_2^2)\\\\\\nabla_{w_2}(w_1+w_2^2)\\end{bmatrix} = \\begin{bmatrix}1\\\\2w_2\\end{bmatrix},$\n","\n"," which at $w_1=1,w_2=2$ is equal to $\\begin{bmatrix}1\\\\4\\end{bmatrix}.$\n","\n"," ---"]},{"cell_type":"code","execution_count":null,"id":"continuous-protocol","metadata":{"id":"continuous-protocol"},"outputs":[],"source":["w1 = tf.Variable(1.0)\n","w2 = tf.Variable(2.0)\n","\n","with tf.GradientTape() as g:\n","    L = w1 + w2**2\n","\n","gradL_w1, gradL_w2 = g.gradient(L, [w1, w2])\n","print('gradL_w1 = ', gradL_w1.numpy(), '; gradL_w2 = ', gradL_w2.numpy())"]},{"cell_type":"markdown","id":"alert-prairie","metadata":{"id":"alert-prairie"},"source":["---\n","\n","In TF, we can control which input is considered an independent variable versus a constant value.\n","\n","`gradient` will return `None` when the input is not a `tf.Variable`.\n","\n","---"]},{"cell_type":"code","execution_count":null,"id":"ordered-midwest","metadata":{"id":"ordered-midwest"},"outputs":[],"source":["# Independent variable\n","w1 = tf.Variable(1.0, name = 'w1')\n","\n","# A tf.constant is not a variable\n","c1 = tf.constant(-2.0, name = 'c1')\n","\n","# Constant because we specify trainable = False\n","c2 = tf.Variable(-1.0, name = 'c2', trainable = False)\n","\n","# variable + tensor returns a tensor. So c3 is not a tf.Variable.\n","c3 = tf.Variable(1.0, name = 'c3') + 1.0\n","\n","# A variable but not used to compute y\n","alpha = tf.Variable(0., name = 'alpha')\n","\n","with tf.GradientTape() as g:\n","    L = (w1 + c1)**2 + (c2**3) + 4*c3\n","\n","grad = g.gradient(L, [w1, c1, c2, c3, alpha])\n","\n","for dw in grad:\n","    print(dw)"]},{"cell_type":"markdown","id":"statewide-voluntary","metadata":{"id":"statewide-voluntary"},"source":["---\n","\n","A `tf.Tensor` can be used as a variable using the `watch` function.\n","\n","For example, consider calculating the sensitivity of $L(w) = w^4$ at $w=-3.$\n","\n","The sensitivity is $\\nabla_wL = 4w^3,$ which at $w=-3$ is equal to $4\\times(-3)^3 = -108.$\n","\n","---"]},{"cell_type":"code","execution_count":null,"id":"stable-macro","metadata":{"id":"stable-macro"},"outputs":[],"source":["w = tf.constant(-3.0)\n","\n","with tf.GradientTape() as g:\n","    g.watch(w)\n","    L = w**4\n","\n","print(g.gradient(L, w))"]},{"cell_type":"markdown","id":"contrary-hollywood","metadata":{"id":"contrary-hollywood"},"source":["---\n","\n","We can use multiple tensor variables as input. This just means we calculate the sensitivity w.r.t. all the variables in the tensor.\n","\n","For example, consider calculating the sensitivity of $L(w) = w[0]^2+w[1]^2$ at $w[0] = 1, w[1] = -3.$\n","\n","---"]},{"cell_type":"code","execution_count":null,"id":"handled-mexico","metadata":{"id":"handled-mexico"},"outputs":[],"source":["w = tf.Variable([1, -3.0])\n","with tf.GradientTape() as g:\n","    L = tf.math.reduce_sum(w**2) # this means L = w[0]^2 + w[1]^2\n","\n","print(w.numpy())\n","print(L.numpy()) # w[0]^2 + w[1]^2 = 1 + 9 = 10\n","print(g.gradient(L, w))  # (2w[0], 2w[1]) = (2,-6)"]},{"cell_type":"markdown","id":"athletic-carnival","metadata":{"id":"athletic-carnival"},"source":["---\n","\n","When `g.gradient` is called with a tensor dependent variable (tensor target), it returns the sum of the sensitivities for each component of the target variable.\n","\n","---"]},{"cell_type":"code","execution_count":null,"id":"fatal-calibration","metadata":{"id":"fatal-calibration"},"outputs":[],"source":["w = tf.Variable(-1.)\n","with tf.GradientTape() as g:\n","    L = [2*w, w**4]\n","\n","print([L[i].numpy() for i in range(2)]) # [-2,1]\n","print(g.gradient(L, w))  # 2 + 4w^3 = 2 - 4 = -2"]},{"cell_type":"markdown","id":"latter-generic","metadata":{"id":"latter-generic"},"source":["---\n","\n","By default, when we call `g.gradient`, all resources required to compute the gradient are released. This allows saving memory. However, there are cases when we want to call `g.gradient` several times, for example, to differentiate different chained functions. In that case, we must use the option `persistent=True`.\n","\n","---"]},{"cell_type":"code","execution_count":null,"id":"conscious-twins","metadata":{"id":"conscious-twins"},"outputs":[],"source":["w1 = tf.Variable([1, -3.0])\n","with tf.GradientTape(persistent = True) as g:\n","    w2 = 2*w1\n","    L = w2**2\n","\n","print(w1.numpy())\n","print(w2.numpy())\n","print(g.gradient(w2, w1))  # [2, 2]\n","print(g.gradient(L, w2))\n","del g # release resources"]},{"cell_type":"markdown","id":"alternate-tournament","metadata":{"id":"alternate-tournament"},"source":["---\n","\n","We can easily calculate sensitivies of functions and plot them.\n","\n","---"]},{"cell_type":"code","execution_count":null,"id":"difficult-documentary","metadata":{"id":"difficult-documentary"},"outputs":[],"source":["w = tf.linspace(-10.0, 10.0, 129) # A tf.Tensor, not a tf.Variable\n","\n","with tf.GradientTape() as g:\n","    g.watch(w)\n","    L = tf.math.tanh(w)\n","\n","gradL_w = g.gradient(L, w)\n","\n","plt.plot(w, L, label = 'L')\n","plt.plot(w, gradL_w, label = 'gradL_w')\n","plt.legend()\n","plt.xlabel('w');"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.9"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}