{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5dEgRpy3952M"
      },
      "outputs": [],
      "source": [
        "## Load libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import sys\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.cm as cm\n",
        "from keras.datasets import mnist\n",
        "plt.style.use('dark_background')\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G9W_1_v_6yq7"
      },
      "outputs": [],
      "source": [
        "np.set_printoptions(precision=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4T7eUtw7Mh0z"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q1e2N5S8MlCU"
      },
      "outputs": [],
      "source": [
        "tf.__version__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "16BpVeIWIOks"
      },
      "source": [
        "---\n",
        "\n",
        "Load MNIST Data\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E5kaKFKSIQgu"
      },
      "outputs": [],
      "source": [
        "## Load MNIST data\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "X_train = X_train.transpose(1, 2, 0)\n",
        "X_test = X_test.transpose(1, 2, 0)\n",
        "X_train = X_train.reshape(X_train.shape[0]*X_train.shape[1], X_train.shape[2])\n",
        "X_test = X_test.reshape(X_test.shape[0]*X_test.shape[1], X_test.shape[2])\n",
        "\n",
        "num_labels = len(np.unique(y_train))\n",
        "num_features = X_train.shape[0]\n",
        "num_samples = X_train.shape[1]\n",
        "\n",
        "# One-hot encode class labels\n",
        "Y_train = tf.keras.utils.to_categorical(y_train).T\n",
        "Y_test = tf.keras.utils.to_categorical(y_test).T\n",
        "\n",
        "\n",
        "# Normalize the samples (images)\n",
        "xmax = np.amax(X_train)\n",
        "xmin = np.amin(X_train)\n",
        "X_train = (X_train - xmin) / (xmax - xmin) # all train features turn into a number between 0 and 1\n",
        "X_test = (X_test - xmin)/(xmax - xmin)\n",
        "\n",
        "print('MNIST set')\n",
        "print('---------------------')\n",
        "print('Number of training samples = %d'%(num_samples))\n",
        "print('Number of features = %d'%(num_features))\n",
        "print('Number of output labels = %d'%(num_labels))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IrXipxwrJ0_8"
      },
      "source": [
        "---\n",
        "\n",
        "A generic layer class with forward and backward methods\n",
        "\n",
        "----"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N4pKUhCyMrWm"
      },
      "outputs": [],
      "source": [
        "class Layer:\n",
        "  def __init__(self):\n",
        "    self.input = None\n",
        "    self.output = None\n",
        "\n",
        "  def forward(self, input):\n",
        "    pass\n",
        "\n",
        "  def backward(self, output_gradient, learning_rate):\n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "CCE loss and its gradient\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "UMt81Faf9-bf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hdXSGW2s7zKd"
      },
      "outputs": [],
      "source": [
        "## Define the loss function and its gradient\n",
        "def cce(Y, Yhat):\n",
        "  return(np.mean(np.sum(-Y*np.log(Yhat), axis = 0)))\n",
        "  #TensorFlow in-built function for categorical crossentropy loss\n",
        "  #cce = tf.keras.losses.CategoricalCrossentropy()\n",
        "  #return(cce(Y, Yhat).numpy())\n",
        "\n",
        "def cce_gradient(Y, Yhat):\n",
        "  return(-Y/Yhat)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "Generic activation layer class\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "jmcNJTjS-BaW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Activation(Layer):\n",
        "    def __init__(self, activation, activation_gradient):\n",
        "        self.activation = ?\n",
        "        self.activation_gradient = activation_gradient\n",
        "\n",
        "    def forward(self, input):\n",
        "        self.input = ?\n",
        "        self.output = self.activation(self.?)\n",
        "        return(self.?)\n",
        "\n",
        "    def backward(self, output_gradient, learning_rate = None):\n",
        "        return(output_gradient[:?, ?] * self.?(self.?))"
      ],
      "metadata": {
        "id": "C21FcWIEwGCN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "Specific activation layer classes\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "JheGWSoKxYWu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Sigmoid(Activation):\n",
        "    def __init__(self):\n",
        "        def sigmoid(z):\n",
        "            return 1 / (1 + np.exp(-z))\n",
        "\n",
        "        def sigmoid_gradient(z):\n",
        "            a = sigmoid(z)\n",
        "            return a * (1 - a)\n",
        "\n",
        "        super().__init__(sigmoid, sigmoid_gradient)\n",
        "\n",
        "class Tanh(Activation):\n",
        "    def __init__(self):\n",
        "        def tanh(z):\n",
        "            return np.tanh(z)\n",
        "\n",
        "        def tanh_gradient(z):\n",
        "            return 1 - np.tanh(z) ** 2\n",
        "\n",
        "        super().__init__(tanh, tanh_gradient)\n",
        "\n",
        "class ReLU(Activation):\n",
        "    def __init__(self):\n",
        "        def relu(z):\n",
        "            return ? * (? > 0)\n",
        "\n",
        "        def relu_gradient(z):\n",
        "            return 1. * (? > 0)\n",
        "\n",
        "        super().__init__(relu, relu_gradient)"
      ],
      "metadata": {
        "id": "PQ5ybz_Yxbef"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "Softmax activation layer class\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "LGdr2m2R-It8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4x1Xn3AbJlNy"
      },
      "outputs": [],
      "source": [
        "## Softmax activation layer class\n",
        "class Softmax(Layer):\n",
        "  def forward(self, input):\n",
        "    self.output = tf.nn.softmax(input, axis = 0).numpy()\n",
        "\n",
        "  def backward(self, output_gradient, learning_rate = None):\n",
        "    ## Following is the inefficient way of calculating the backward gradient\n",
        "    softmax_gradient = np.empty((self.output.shape[0], output_gradient.shape[1]), dtype = np.float64)\n",
        "    for b in range(softmax_gradient.shape[1]):\n",
        "      softmax_gradient[:, b] = np.dot((np.identity(self.output.shape[0])-np.atleast_2d(self.output[:, b])) * np.atleast_2d(self.output[:, b]).T, output_gradient[:, b])\n",
        "\n",
        "    # Return gradient w.r.t. input for backward propagation\n",
        "    return(softmax_gradient)\n",
        "\n",
        "    ## Following is the efficient way of calculating the backward gradient\n",
        "    #T = np.transpose(np.identity(self.output.shape[0]) - np.atleast_2d(self.output).T[:, np.newaxis, :], (2, 1, 0)) * np.atleast_2d(self.output)\n",
        "    #return(np.einsum('jik, ik -> jk', T, output_gradient))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "Dense layer class\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "UKnqi7rf-MBn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ctXhZYCTmHK"
      },
      "outputs": [],
      "source": [
        "## Dense layer class\n",
        "class Dense(Layer):\n",
        "    def __init__(self, input_size, output_size):\n",
        "        self.weights = 0.01*np.random.randn(output_size, input_size+1) # bias trick\n",
        "        self.weights[:, -1] = 0.01 # set all bias values to the same nonzero constant\n",
        "\n",
        "    def forward(self, input):\n",
        "        self.input = np.vstack([input, np.ones((1, input.shape[1]))]) # bias trick\n",
        "        self.output= np.dot(self.weights, self.input)\n",
        "\n",
        "    def backward(self, output_gradient, learning_rate):\n",
        "        ## Following is the inefficient way of calculating the gradient w.r.t. weights\n",
        "        weights_gradient = np.zeros((self.output.shape[0], self.input.shape[0]), dtype = np.float64)\n",
        "        for b in range(output_gradient.shape[1]):\n",
        "          weights_gradient += np.dot(output_gradient[:, b].reshape(-1, 1), self.input[:, b].reshape(-1, 1).T)\n",
        "        weights_gradient = (1/output_gradient.shape[1])*weights_gradient\n",
        "\n",
        "        ## Following is the efficient way of calculating the weights gradient\n",
        "        #weights_gradient = (1/output_gradient.shape[1])*np.dot(np.atleast_2d(output_gradient), np.atleast_2d(self.input).T)\n",
        "\n",
        "        # Gradient w.r.t. the input\n",
        "        input_gradient = np.dot(self.?.T, ?)\n",
        "\n",
        "        # Update weights using gradient descent step\n",
        "        self.weights = self.weights + learning_rate * (-dense_gradient)\n",
        "\n",
        "        # Return gradient w.r.t. input for backward propagation\n",
        "        return(input_gradient)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2W1howeOJegI"
      },
      "source": [
        "---\n",
        "\n",
        "Function to generate sample indices for batch processing according to batch size\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MHyjEf22IRpc"
      },
      "outputs": [],
      "source": [
        "## Function to generate sample indices for batch processing according to batch size\n",
        "def generate_batch_indices(num_samples, batch_size):\n",
        "  # Reorder sample indices\n",
        "  reordered_sample_indices = np.random.choice(num_samples, num_samples, replace = False)\n",
        "  # Generate batch indices for batch processing\n",
        "  batch_indices = np.split(reordered_sample_indices, np.arange(batch_size, len(reordered_sample_indices), batch_size))\n",
        "  return(batch_indices)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fI_Gms9fJqbs"
      },
      "source": [
        "---\n",
        "\n",
        "Train the 1-hidden layer neural network (128 nodes) using batch training with batch size = 100\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LGIzrN-rPuI4"
      },
      "outputs": [],
      "source": [
        "## Train the 1-hidden layer neural network (128 nodes)\n",
        "## using batch training with batch size = 100\n",
        "learning_rate = 1e-3 # learning rate\n",
        "batch_size = ? # batch size\n",
        "nepochs = 200 # number of epochs\n",
        "loss_epoch = np.empty(nepochs, dtype = np.float64) # create empty array to store losses over each epoch\n",
        "\n",
        "# Neural network architecture\n",
        "\n",
        "dlayer1 = Dense(?, ?) # define dense layer 1\n",
        "alayer1 = ReLU() # ReLU activation layer 1\n",
        "dlayer2 = Dense(?, ?) # define dense layer 2\n",
        "softmax = Softmax() # define softmax activation layer\n",
        "\n",
        "# Steps: run over each sample in the batch, calculate loss, gradient of loss,\n",
        "# and update weights.\n",
        "\n",
        "epoch = 0\n",
        "while epoch < nepochs:\n",
        "  batch_indices = generate_batch_indices(num_samples, batch_size)\n",
        "  loss = 0\n",
        "  for b in range(len(batch_indices)):\n",
        "    dlayer1.forward(X_train[:, batch_indices[b]]) # forward prop dense layer 1 with batch feature added\n",
        "    alayer1.forward(?) # forward prop activation layer 1\n",
        "    dlayer2.forward(?) # forward prop dense layer 2\n",
        "    softmax.forward(?) # Softmax activate\n",
        "    loss += cce(Y_train[:, batch_indices[b]], softmax.output) # calculate loss\n",
        "    # Backward prop starts here\n",
        "    grad = cce_gradient(Y_train[:, batch_indices[b]], softmax.output)\n",
        "    grad = softmax.backward(?)\n",
        "    grad = dlayer2.backward(? learning_rate)\n",
        "    grad = alayer1.backward(?)\n",
        "    grad = dlayer1.backward(?, learning_rate)\n",
        "  loss_epoch[epoch] = loss/len(batch_indices)\n",
        "  print('Epoch %d: loss = %f'%(epoch+1, loss_epoch[epoch]))\n",
        "  epoch = epoch + 1"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "Plot training loss vs. epoch\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "EhXEFASk-Tkv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Iv3k23SlCqGf"
      },
      "outputs": [],
      "source": [
        "# Plot training loss as a function of epoch:\n",
        "plt.plot(loss_epoch)\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss value')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "Test performance on test data\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "TaLoOOWK-WBj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d7AEbmpcKcPY"
      },
      "outputs": [],
      "source": [
        "dlayer1.forward(X_test)\n",
        "alayer1.forward(dlayer1.output)\n",
        "dlayer2.forward(alayer1.output)\n",
        "softmax.forward(dlayer2.output)\n",
        "ypred = np.argmax(softmax.output.T, axis = 1)\n",
        "print(ypred)\n",
        "ytrue = np.argmax(Y_test.T, axis = 1)\n",
        "print(ytrue)\n",
        "np.mean(ytrue == ypred)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}