{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "**Load essential libraries**\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "CrW3wGfQN2KV"
      },
      "id": "CrW3wGfQN2KV"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "valued-lodging",
      "metadata": {
        "id": "valued-lodging"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "plt.style.use('dark_background')\n",
        "%matplotlib inline\n",
        "\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "**Check TensorFlow version**\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "CZmMj92lN6Yc"
      },
      "id": "CZmMj92lN6Yc"
    },
    {
      "cell_type": "code",
      "source": [
        "tf.__version__"
      ],
      "metadata": {
        "id": "B2VpALYvOBoG"
      },
      "id": "B2VpALYvOBoG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "Load MNIST Data\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "8jG5ivrRX1O-"
      },
      "id": "8jG5ivrRX1O-"
    },
    {
      "cell_type": "code",
      "source": [
        "## Load MNIST data\n",
        "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "X_train = X_train.reshape(X_train.shape[0], X_train.shape[1]*X_train.shape[2])\n",
        "X_test = X_test.reshape(X_test.shape[0], X_test.shape[1]*X_test.shape[2])\n",
        "\n",
        "num_labels = len(np.unique(y_train))\n",
        "num_features = X_train.shape[1]\n",
        "num_samples = X_train.shape[0]\n",
        "\n",
        "# One-hot encode class labels\n",
        "Y_train = tf.keras.utils.to_categorical(y_train)\n",
        "Y_test = tf.keras.utils.to_categorical(y_test)\n",
        "\n",
        "# Normalize the samples (images)\n",
        "xmax = np.amax(X_train)\n",
        "xmin = np.amin(X_train)\n",
        "X_train = (X_train - xmin) / (xmax - xmin) # all train features turn into a number between 0 and 1\n",
        "X_test = (X_test - xmin)/(xmax - xmin)\n",
        "\n",
        "print('MNIST set')\n",
        "print('---------------------')\n",
        "print('Number of training samples = %d'%(num_samples))\n",
        "print('Number of features = %d'%(num_features))\n",
        "print('Number of output labels = %d'%(num_labels))"
      ],
      "metadata": {
        "id": "qs1J2W_qX0xw"
      },
      "id": "qs1J2W_qX0xw",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "We will now look at 3 different ways to build custom models using TensorFlow 2:\n",
        "\n",
        "1. model subclassing ([Making new layers and models via subclassing](https://www.tensorflow.org/guide/keras/making_new_layers_and_models_via_subclassing))\n",
        "2. sequential API\n",
        "3. functional API\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "kjk0YnbgGsr8"
      },
      "id": "kjk0YnbgGsr8"
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "**Approach-1**: here we build the model by subclassing the Keras $\\texttt{Model}$ class followed by definition of of layers in $\\texttt{__init__}$ and implementation of the model's forward pass in $\\texttt{call()}$.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "MRgFmy-mYgAS"
      },
      "id": "MRgFmy-mYgAS"
    },
    {
      "cell_type": "code",
      "source": [
        "## Define 1-layer (softmax) neural network architecture\n",
        "# Define model\n",
        "class Softmax_Model(tf.keras.models.Model):\n",
        "    def __init__(self):\n",
        "        super(Softmax_Model, self).__init__()\n",
        "        initializer = tf.keras.initializers.RandomUniform(minval=-0.5, maxval=0.5)\n",
        "        self.dense1 = tf.keras.layers.Dense(num_labels, dtype = 'float64',\\\n",
        "                                 bias_initializer = initializer,\\\n",
        "                                 activation = tf.keras.activations.softmax)\n",
        "\n",
        "    # Forward pass for the model\n",
        "    def call(self, inputs):\n",
        "        a = self.dense1(inputs)\n",
        "        return a"
      ],
      "metadata": {
        "id": "Z_u1srfaY1Tc"
      },
      "id": "Z_u1srfaY1Tc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "Build model\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "EZyM0MekDtFI"
      },
      "id": "EZyM0MekDtFI"
    },
    {
      "cell_type": "code",
      "source": [
        "## Build model\n",
        "model = Softmax_Model()\n",
        "batch_size = 100 # batch size\n",
        "model.build((batch_size, num_features))"
      ],
      "metadata": {
        "id": "qfPaz4la4xfe"
      },
      "id": "qfPaz4la4xfe",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "Compile and train the model on the training batches and test on the test set in one shot\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "od-XJBv8DyiL"
      },
      "id": "od-XJBv8DyiL"
    },
    {
      "cell_type": "code",
      "source": [
        "## Compile model\n",
        "opt = tf.keras.optimizers.Adam(learning_rate = 1e-03) # optimizer\n",
        "loss_fn = tf.keras.losses.CategoricalCrossentropy()  # loss function\n",
        "model.compile(optimizer = opt, loss = loss_fn, metrics = ['acc'])\n",
        "\n",
        "# Train model and simultabeously test on the test set\n",
        "model.fit(X_train, Y_train, batch_size = 100,\\\n",
        "          epochs = 10,\\\n",
        "          validation_data = (X_test, Y_test))"
      ],
      "metadata": {
        "id": "eP63ueEF9Ist"
      },
      "id": "eP63ueEF9Ist",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "Instead of doing the above, we can explicitly write down the optimization step using $\\texttt{GradientTape()}$ and train the model\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "k0H0scBfV3wH"
      },
      "id": "k0H0scBfV3wH"
    },
    {
      "cell_type": "code",
      "source": [
        "## Create source dataset from input data (this is helpful for ppipelining later)\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((X_train, Y_train))\n",
        "batch_size = 100 # batch size\n",
        "# Create training batches\n",
        "train_dataset = train_dataset.shuffle(buffer_size = 1024).batch(batch_size)"
      ],
      "metadata": {
        "id": "SW3BgQ3EW56s"
      },
      "id": "SW3BgQ3EW56s",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create softmax model\n",
        "model = Softmax_Model()\n",
        "\n",
        "opt = tf.keras.optimizers.Adam(learning_rate = 1e-03) # optimizer\n",
        "loss_fn = tf.keras.losses.CategoricalCrossentropy()  # loss function\n",
        "\n",
        "# Varible to store training loss per epoch\n",
        "loss_train_epoch = tf.keras.metrics.Mean()\n",
        "\n",
        "# Iterate over epochs\n",
        "nepochs = 10\n",
        "for epoch in range(nepochs):\n",
        "  # Iterate over the batches of the dataset.\n",
        "  for step, train_batch in enumerate(train_dataset):\n",
        "    with tf.GradientTape() as g:\n",
        "      # Compute loss\n",
        "      yhat = model(train_batch[0])\n",
        "      loss = loss_fn(train_batch[1], yhat)\n",
        "\n",
        "    # Calculate gradients\n",
        "    grad = g.gradient(loss, model.trainable_weights)\n",
        "\n",
        "    # Update model\n",
        "    opt.apply_gradients(zip(grad, model.trainable_weights))\n",
        "\n",
        "    # Append training loss\n",
        "    loss_train_epoch(loss)\n",
        "  print('Epoch %d: train loss = %f'%(epoch+1, loss_train_epoch.result()))"
      ],
      "metadata": {
        "id": "2eqel_RYgMR9"
      },
      "id": "2eqel_RYgMR9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile model so it can be evaluated for test set\n",
        "model.compile(optimizer = opt, loss = loss_fn, metrics = ['acc'])\n",
        "print('\\nAccuracy:', model.evaluate(X_test, Y_test, verbose=0)[1])"
      ],
      "metadata": {
        "id": "0zoDASIQs8T4"
      },
      "id": "0zoDASIQs8T4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "**Approach-2**: here we build the model using the sequential API of TensorFlow Keras. Try this.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "040kYit81aFc"
      },
      "id": "040kYit81aFc"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Wmu_m6kl1jci"
      },
      "id": "Wmu_m6kl1jci",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "**Approach-3**: here we build the model using the functional API of TensorFlow Keras. Try this.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "Ljcm4VTm1j9f"
      },
      "id": "Ljcm4VTm1j9f"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MMVOs9Jk1noX"
      },
      "id": "MMVOs9Jk1noX",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}