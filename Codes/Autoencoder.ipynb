{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5dEgRpy3952M"
      },
      "outputs": [],
      "source": [
        "## Load libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import sys\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.cm as cm\n",
        "from sklearn.decomposition import PCA\n",
        "from tensorflow.keras.datasets import mnist\n",
        "plt.style.use('dark_background')\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G9W_1_v_6yq7"
      },
      "outputs": [],
      "source": [
        "np.set_printoptions(precision=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4T7eUtw7Mh0z"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q1e2N5S8MlCU"
      },
      "outputs": [],
      "source": [
        "tf.__version__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "16BpVeIWIOks"
      },
      "source": [
        "---\n",
        "\n",
        "Load MNIST Data\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E5kaKFKSIQgu"
      },
      "outputs": [],
      "source": [
        "## Load MNIST data\n",
        "(X_train, _), (X_test, _) = mnist.load_data()\n",
        "X_train =X_train.reshape(X_train.shape[0], X_train.shape[1]*X_train.shape[2])\n",
        "X_test = X_test.reshape(X_test.shape[0], X_test.shape[1]*X_test.shape[2])\n",
        "\n",
        "num_features = X_train.shape[1]\n",
        "num_samples = X_train.shape[0]\n",
        "\n",
        "# Normalize the samples (images)\n",
        "xmax = np.amax(X_train)\n",
        "xmin = np.amin(X_train)\n",
        "X_train = (X_train - xmin) / (xmax - xmin) # all train features turn into a number between 0 and 1\n",
        "X_test = (X_test - xmin)/(xmax - xmin)\n",
        "\n",
        "print('MNIST set')\n",
        "print('---------------------')\n",
        "print('Number of training samples = %d'%(num_samples))\n",
        "print('Number of features = %d'%(num_features))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Parameters for the autoencoder\n",
        "batch_size = 256\n",
        "max_epochs = 50\n",
        "learning_rate = 1e-03\n",
        "latent_dim = 128\n",
        "hidden_dim = 256\n",
        "original_dim = X_train.shape[1]"
      ],
      "metadata": {
        "id": "88RL0fqQ1HTm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Convert numpy to tf.data.datasets\n",
        "training_dataset = tf.data.Dataset.from_tensor_slices(X_train).batch(batch_size)"
      ],
      "metadata": {
        "id": "oEd_W9b02YzY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Encoder\n",
        "class Encoder(tf.keras.layers.Layer):\n",
        "  # Define input independent model information\n",
        "  def __init__(self, hidden_dim, latent_dim):\n",
        "    super(Encoder, self).__init__()\n",
        "    self.encoder_layer1 = tf.keras.layers.Dense(units = hidden_dim, activation = tf.nn.relu)\n",
        "    self.encoder_layer2 = tf.keras.layers.Dense(units = latent_dim, activation = tf.nn.relu)\n",
        "\n",
        "  ## Method for forward propagation\n",
        "  def call(self, input_features):\n",
        "    a = self.encoder_layer1(input_features)\n",
        "    a = self.encoder_layer2(a)\n",
        "    return a"
      ],
      "metadata": {
        "id": "cl4Pou1Z3YA9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Decoder\n",
        "class Decoder(tf.keras.layers.Layer):\n",
        "  def __init__(self, latent_dim, hidden_dim, original_dim):\n",
        "    super(Decoder, self).__init__()\n",
        "    self.decoder_layer1 = tf.keras.layers.Dense(units = hidden_dim, activation = tf.nn.relu)\n",
        "    self.decoder_layer2 = tf.keras.layers.Dense(units = original_dim, activation = tf.nn.relu)\n",
        "\n",
        "  def call(self, encoded_features):\n",
        "    a = self.decoder_layer1(encoded_features)\n",
        "    a = self.decoder_layer2(a)\n",
        "    return a"
      ],
      "metadata": {
        "id": "KPHSVJu657NA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Autoencoder\n",
        "class Autoencoder(tf.keras.Model):\n",
        "  def __init__(self, latent_dim, hidden_dim, original_dim):\n",
        "    super(Autoencoder, self).__init__()\n",
        "    self.loss = []\n",
        "    self.encoder = Encoder(hidden_dim = hidden_dim, latent_dim = latent_dim)\n",
        "    self.decoder = Decoder(latent_dim = latent_dim, hidden_dim = hidden_dim, original_dim = original_dim)\n",
        "\n",
        "  def call(self, input_features):\n",
        "    encoded_features = self.encoder(input_features)\n",
        "    reconstructed_features = self.decoder(encoded_features)\n",
        "    return reconstructed_features"
      ],
      "metadata": {
        "id": "WRprWraw7uQJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Build model\n",
        "autoencoder = Autoencoder(latent_dim = latent_dim,\n",
        "                          hidden_dim = hidden_dim,\n",
        "                          original_dim = original_dim)"
      ],
      "metadata": {
        "id": "ZaNbEZdK_hp1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Optimizer\n",
        "opt = tf.keras.optimizers.Adam(learning_rate = learning_rate)"
      ],
      "metadata": {
        "id": "8DvpkLj8_85o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Custom training - loss\n",
        "def loss(true, pred):\n",
        "  return tf.reduce_mean(tf.square(tf.subtract(true, pred)))"
      ],
      "metadata": {
        "id": "vvPXu-SIAa4S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Custom training - compute gradient of loss and update weights\n",
        "@tf.function\n",
        "def train(model, loss, opt, original_features):\n",
        "  with tf.GradientTape() as g:\n",
        "    pred = tf.cast(model(original_features), tf.float64)\n",
        "    loss_batch = loss(original_features, pred)\n",
        "  gradients = g.gradient(loss_batch, model.trainable_variables)\n",
        "  opt.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "  return loss_batch"
      ],
      "metadata": {
        "id": "95bPW9eEBDUL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Train network\n",
        "# Varible to store training loss per epoch\n",
        "loss_train_epoch = tf.keras.metrics.Mean()\n",
        "loss_train_epoch_plot = np.empty(max_epochs)\n",
        "\n",
        "# Iterate over epochs\n",
        "for epoch in range(max_epochs):\n",
        "  for step, train_batch_features in enumerate(training_dataset):\n",
        "    loss_batch = train(autoencoder, loss, opt, train_batch_features)\n",
        "    # Append training loss\n",
        "    loss_train_epoch(loss_batch)\n",
        "  loss_train_epoch_plot[epoch] = loss_train_epoch.result().numpy()\n",
        "  print(f'Epoch {epoch+1}, loss = {loss_train_epoch_plot[epoch]}')"
      ],
      "metadata": {
        "id": "CZKUl3VMCs_U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot train loss as a function of epoch:\n",
        "fig, ax = plt.subplots(1, 1, figsize = (4, 4))\n",
        "fig.tight_layout(pad = 4.0)\n",
        "ax.plot(loss_train_epoch_plot, 'b')\n",
        "ax.set_xlabel('Epoch', fontsize = 12)\n",
        "ax.set_ylabel('Loss value', fontsize = 12)\n",
        "ax.set_xticks(np.arange(0, max_epochs+1, 5))\n",
        "ax.set_title('Loss vs. Epoch', fontsize = 14)"
      ],
      "metadata": {
        "id": "boQTM4NqQyH3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot random input images and their reconstructed versions\n",
        "img = {j:index for j,\n",
        "       index in zip(range(5),\n",
        "                    np.random.choice(X_test.shape[0], 5))}\n",
        "fig, ax = plt.subplots(2, 5, figsize = (8, 2))\n",
        "for j, ind in img.items():\n",
        "  # Original image\n",
        "  ax[0, j].imshow(X_test[ind].reshape(28, 28), cmap = 'gray')\n",
        "  # Reconstructed image\n",
        "  ax[1, j].imshow(autoencoder(X_test[ind].reshape(1, -1)).numpy().reshape(28, 28), cmap = 'gray')"
      ],
      "metadata": {
        "id": "SxEF9u9G_Ord"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Dimension reduction and reconstruction using PCA\n",
        "# Create and fit PCA object on the training data\n",
        "k = 200 # we will look the top 10 eigenvectors\n",
        "pca = PCA(n_components = 20)\n",
        "# Fit PCA using training data\n",
        "pca.fit(X_train)\n",
        "# Reconstruct the random test images chosen in the previous cell\n",
        "X_test_reconstructed = pca.inverse_transform(pca.transform(X_test[list(img.values())]))\n",
        "\n",
        "fig, ax = plt.subplots(2, 5, figsize = (8, 2))\n",
        "for j in range(X_test_reconstructed.shape[0]):\n",
        "  # Original image\n",
        "  ax[0, j].imshow(X_test[img[j]].reshape(28, 28), cmap = 'gray')\n",
        "  # Reconstructed image\n",
        "  ax[1, j].imshow(X_test_reconstructed[j].reshape(28, 28), cmap = 'gray')"
      ],
      "metadata": {
        "id": "wT8sai-QSMb3"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}